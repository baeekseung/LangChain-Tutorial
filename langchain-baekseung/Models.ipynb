{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 다양한 LLM 모델 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API KEY를 환경변수로 관리하기 위한 설정 파일\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# API KEY 정보로드\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangSmith 추적을 설정합니다. https://smith.langchain.com\n",
    "# !pip install langchain-teddynote\n",
    "from langchain_teddynote import logging\n",
    "from langchain_teddynote.messages import stream_response\n",
    "\n",
    "# 프로젝트 이름을 입력합니다.\n",
    "logging.langsmith(\"CH04-Models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI\n",
    "\n",
    "### 개요\n",
    "OpenAI는 채팅 전용 Large Language Model (LLM)을 제공합니다. 이 모델을 생성할 때 다양한 옵션을 지정할 수 있으며, 이러한 옵션들은 모델의 동작 방식에 영향을 미칩니다.\n",
    "\n",
    "### 옵션 상세 설명\n",
    "\n",
    "`temperature`\n",
    "\n",
    "- 샘플링 온도를 설정하는 옵션입니다. 값은 0과 2 사이에서 선택할 수 있습니다. 높은 값(예: 0.8)은 출력을 더 무작위하게 만들고, 낮은 값(예: 0.2)은 출력을 더 집중되고 결정론적으로 만듭니다.\n",
    "\n",
    "`max_tokens`\n",
    "\n",
    "- 채팅 완성에서 생성할 토큰의 최대 개수를 지정합니다. 이 옵션은 모델이 한 번에 생성할 수 있는 텍스트의 길이를 제어합니다.\n",
    "\n",
    "`model_name`\n",
    "\n",
    "- 적용 가능한 모델을 선택하는 옵션입니다. 더 자세한 정보는 [OpenAI 모델 문서](https://platform.openai.com/docs/models)에서 확인할 수 있습니다.\n",
    "\n",
    "\n",
    "**모델 스펙**\n",
    "\n",
    "- 링크: https://platform.openai.com/docs/models/gpt-4o\n",
    "\n",
    "| 모델명 | 설명 | 컨텍스트 길이 | 최대 출력 토큰 | 학습 데이터 |\n",
    "|--------|------|---------------|-----------------|-------------|\n",
    "| gpt-4o | GPT-4 터보보다 저렴하고 빠른 최신 다중모드 플래그십 모델 | 128,000 토큰 | 4,096 토큰 | 2023년 10월까지 |\n",
    "| gpt-4-turbo | 최신 GPT-4 터보 모델. 비전 기능, JSON 모드, 기능 호출 지원 | 128,000 토큰 | 4,096 토큰 | 2023년 12월까지 |\n",
    "| gpt-4o-mini | GPT-3.5 터보보다 더 우수한 성능의 작은 모델 | 128,000 토큰 | 16,384 토큰 | 2023년 10월까지 |\n",
    "| o1-preview | 다양한 도메인의 어려운 문제 해결을 위한 추론 모델 | 128,000 토큰 | 32,768 토큰 | 2023년 10월까지 |\n",
    "| o1-mini | 코딩, 수학, 과학에 특화된 빠른 추론 모델 | 128,000 토큰 | 65,536 토큰 | 2023년 10월까지 |\n",
    "| gpt-4o-realtime | 실시간 API용 오디오 및 텍스트 입력 처리 모델 (베타) | 128,000 토큰 | 4,096 토큰 | 2023년 10월까지 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# ChatOpenAI 객체를 생성합니다.\n",
    "gpt = ChatOpenAI(\n",
    "    temperature=0,\n",
    "    model_name=\"gpt-4o\",  # 모델명\n",
    ")\n",
    "\n",
    "# 스트리밍 출력을 위하여 invoke() 대신 stream()을 사용합니다.\n",
    "answer = gpt.stream(\"사랑이 뭔가요?\")\n",
    "\n",
    "# 답변 출력\n",
    "stream_response(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anthropic\n",
    "\n",
    "Anthropic은 인공지능(AI) 안전성과 연구에 중점을 둔 미국의 스타트업 기업입니다. 주요 정보는 다음과 같습니다:\n",
    "\n",
    "- **설립 연도**: 2021년\n",
    "- **위치**: 미국 샌프란시스코\n",
    "- **창립자**: OpenAI 출신 직원들 (Daniela Amodei와 Dario Amodei 등)\n",
    "- **기업 형태**: 공익기업(Public Benefit Corporation)으로 등록\n",
    "\n",
    "## Claude\n",
    "\n",
    "Claude는 Anthropic의 대표적인 대규모 언어 모델(LLM) 제품군입니다. \n",
    "\n",
    "- **API 키 발급**: [https://console.anthropic.com/settings/keys](https://console.anthropic.com/settings/keys)\n",
    "- **모델 리스트**: [https://docs.anthropic.com/en/docs/about-claude/models](https://docs.anthropic.com/en/docs/about-claude/models)\n",
    "\n",
    "\n",
    "![](images/anthropic-20241023.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| model_name        | model                              | Anthropic API                                | AWS Bedrock                                   | GCP Vertex AI                      |\n",
    "|------------------|------------------------------------|--------------------------------------------|----------------------------------------------|-----------------------------------|\n",
    "| Claude 3.5 Opus  | 연말 출시 예정                         | 연말 출시 예정                                | 연말 출시 예정                                   | 연말 출시 예정                        |\n",
    "| Claude 3.5 Sonnet| claude-3-5-sonnet-20241022         | claude-3-5-sonnet-20241022                 | anthropic.claude-3-5-sonnet-20241022-v2:0     | claude-3-5-sonnet-v2@20241022      |\n",
    "| Claude 3.5 Haiku | 연말 출시 예정                         | 연말 출시 예정                                | 연말 출시 예정                                   | 연말 출시 예정                        |\n",
    "| Claude 3 Opus    | claude-3-opus-20240229             | claude-3-opus-20240229                     | anthropic.claude-3-opus-20240229-v1:0         | claude-3-opus@20240229             |\n",
    "| Claude 3 Sonnet  | claude-3-sonnet-20240229           | claude-3-sonnet-20240229                   | anthropic.claude-3-sonnet-20240229-v1:0       | claude-3-sonnet@20240229           |\n",
    "| Claude 3 Haiku   | claude-3-haiku-20240307            | claude-3-haiku-20240307                    | anthropic.claude-3-haiku-20240307-v1:0        | claude-3-haiku@20240307            |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "# ChatAnthropic 객체를 생성합니다.\n",
    "anthropic = ChatAnthropic(model_name=\"claude-3-5-sonnet-20241022\")\n",
    "\n",
    "# 스트리밍 출력을 위하여 invoke() 대신 stream()을 사용합니다.\n",
    "answer = anthropic.stream(\"사랑이 뭔가요?\")\n",
    "\n",
    "# 답변 출력\n",
    "stream_response(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perplexity\n",
    "\n",
    "링크: https://www.perplexity.ai/\n",
    "\n",
    "- **설립연도**: 2022년\n",
    "- **주요 투자자**: Jeff Bezos, Nvidia, Databricks, Bessemer Venture Partners, IVP, Wayra 등\n",
    "- **최근 펀딩**: 5억 달러 (2024년 10월)\n",
    "- **기업 가치**: 약 90억 달러 (2024년 11월 기준)\n",
    "- **월간 활성 사용자**: 1,500만 명\n",
    "\n",
    "## Perplexity Pro 정확한 특징\n",
    "\n",
    "- **일일 Pro 검색**: 300회\n",
    "- **AI 모델 선택**: GPT-4 Omni, Claude 3 Sonnet/Haiku, Sonar Large 32k[5]\n",
    "- **파일 분석**: PDF, CSV, 이미지 파일 지원\n",
    "- **가격**: 월 $20 또는 연 $200\n",
    "\n",
    "## Perplexity API 사용 방법\n",
    "\n",
    "**가격**\n",
    "\n",
    "![perplexity-pricing.png](attachment:perplexity-pricing.png)\n",
    "\n",
    "- **API 크레딧 획득**\n",
    "   - Perplexity Pro 구독 시 **매월 $5 상당의 API 크레딧 제공**\n",
    "\n",
    "- **API 모델 옵션**\n",
    "   - Llama 3 기반 모델\n",
    "   - Perplexity 온라인 LLM\n",
    "   - 인용 기능 포함\n",
    "\n",
    "- API 키 발급: [API 콘솔](https://www.perplexity.ai/settings/api)\n",
    "\n",
    "API 키 발급 후 `.env` 파일에 키 저장\n",
    "\n",
    "```\n",
    "PPLX_API_KEY=이곳에 API 키를 입력하세요.\n",
    "```\n",
    "\n",
    "혹은\n",
    "\n",
    "```\n",
    "import os\n",
    "\n",
    "os.environ[\"PPLX_API_KEY\"] = \"이곳에 API 키를 입력하세요.\"\n",
    "```\n",
    "\n",
    "**참고**\n",
    "\n",
    "- [API 문서](https://docs.perplexity.ai/api-reference/chat-completions)\n",
    "\n",
    "## 지원 모델\n",
    "\n",
    "| Model                                 | Parameter Count | Context Length | Model Type        |\n",
    "|---------------------------------------|-----------------|----------------|-------------------|\n",
    "| `llama-3.1-sonar-small-128k-online`   | 8B              | 127,072        | Chat Completion   |\n",
    "| `llama-3.1-sonar-large-128k-online`   | 70B             | 127,072        | Chat Completion   |\n",
    "| `llama-3.1-sonar-huge-128k-online`    | 405B            | 127,072        | Chat Completion   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ChatPerplexity 매개변수\n",
    "\n",
    "`model`  \n",
    "사용할 언어 모델을 지정 (예: \"llama-3.1-sonar-small-128k-online\") - 기본 성능과 능력을 결정.\n",
    "\n",
    "`temperature`  \n",
    "응답의 무작위성을 조절 (0.0-1.0), 0은 결정적, 1은 가장 무작위한 응답 생성.\n",
    "\n",
    "`top_p`  \n",
    "토큰 샘플링의 확률 임계값 설정 (0.0-1.0), 높을수록 더 다양한 출력 허용.\n",
    "\n",
    "`search_domain_filter`  \n",
    "검색 결과를 지정된 도메인으로 제한, 리스트 형태로 제공 (예: [\"perplexity.ai\"]).\n",
    "\n",
    "`return_images`  \n",
    "응답에 이미지 포함 여부를 결정하는 불리언 플래그.\n",
    "\n",
    "`return_related_questions`  \n",
    "관련 질문 제안 기능을 활성화/비활성화하는 불리언 플래그.\n",
    "\n",
    "`top_k`  \n",
    "사용할 검색 결과의 수 제한 (0은 제한 없음을 의미).\n",
    "\n",
    "`streaming`  \n",
    "응답을 스트리밍으로 받을지 완성된 형태로 받을지 결정하는 불리언 플래그.\n",
    "\n",
    "`presence_penalty`  \n",
    "토큰 반복에 대한 페널티 (-2.0에서 2.0), 높을수록 재사용을 억제.\n",
    "\n",
    "`frequency_penalty`  \n",
    "일반적/희귀 토큰 선호도 조정 (-2.0에서 2.0), 높을수록 희귀 토큰 선호."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_teddynote.models import ChatPerplexity\n",
    "\n",
    "perplexity = ChatPerplexity(\n",
    "    model=\"llama-3.1-sonar-large-128k-online\",\n",
    "    temperature=0.2,\n",
    "    top_p=0.9,\n",
    "    search_domain_filter=[\"perplexity.ai\"],\n",
    "    return_images=False,\n",
    "    return_related_questions=True,\n",
    "    # search_recency_filter=\"month\",\n",
    "    top_k=0,\n",
    "    streaming=False,\n",
    "    presence_penalty=0,\n",
    "    frequency_penalty=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "응답을 출력합니다. `ChatPerplexity` 는 지식 정보의 출처를 `citations` 속성에 저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 응답 출력\n",
    "response = perplexity.invoke(\"2024년 노벨문학상 수상자를 조사해 주세요\")\n",
    "print(response.content)\n",
    "\n",
    "print()\n",
    "for i, citation in enumerate(response.citations):\n",
    "    print(f\"[{i+1}] {citation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "스트리밍 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = perplexity.stream(\"2024년 노벨문학상 수상자를 조사해 주세요\")\n",
    "\n",
    "for token in response:\n",
    "    print(token.content, end=\"\", flush=True)\n",
    "\n",
    "print(\"\\n\")\n",
    "for i, citation in enumerate(token.citations):\n",
    "    print(f\"[{i+1}] {citation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Together AI\n",
    "\n",
    "- 링크: https://www.together.ai/\n",
    "- API 키 발급: https://api.together.ai/\n",
    "- `TOGETHER_API_KEY` 환경변수 설정\n",
    "  \n",
    "Together AI는 2022년 샌프란시스코에서 설립된 생성형 AI 클라우드 플랫폼 회사입니다(NVIDIA, Kleiner Perkins, Lux, NEA 등으로부터 1억 2천만 달러 이상의 투자를 유치했습니다).\n",
    "\n",
    "### 주요 제품과 특징\n",
    "\n",
    "**Together Inference**\n",
    "\n",
    "- 업계에서 가장 빠른 추론 스택을 제공하며, vLLM보다 최대 4배 빠른 성능을 보입니다\n",
    "- Llama-3 70B 사용 시 GPT-4 대비 11배 낮은 비용으로 운영 가능\n",
    "- 자동 확장 기능으로 API 요청 볼륨에 맞춰 용량이 자동으로 조정됩니다\n",
    "\n",
    "**Together Custom Models**\n",
    "\n",
    "- 사용자 맞춤형 AI 모델 학습 및 Fine-tuning 지원\n",
    "- FlashAttention-3와 같은 최신 최적화 기술 적용\n",
    "- 학습된 모델에 대한 완전한 소유권 보장\n",
    "\n",
    "### 기술적 특징\n",
    "\n",
    "**성능 최적화**\n",
    "\n",
    "- FlashAttention-3 커널과 독점 커널을 통합한 추론 엔진 보유\n",
    "- Medusa와 SpecExec 같은 추측적 디코딩 알고리즘 적용\n",
    "- 최고의 정확도와 성능을 위한 독자적인 양자화 기술 사용\n",
    "\n",
    "**지원 모델**\n",
    "\n",
    "- Google Gemma, Meta의 Llama 3.3, Qwen2.5, Mistral AI의 Mistral/Mixtral 등 200개 이상의 오픈소스 모델 지원\n",
    "- 멀티모달 AI 모델 지원으로 다양한 형태의 데이터 처리 가능\n",
    "\n",
    "**보안 및 프라이버시**\n",
    "\n",
    "- 사용자가 명시적으로 동의하지 않는 한 데이터는 새로운 모델 학습에 사용되지 않음\n",
    "- 데이터 보관에 대한 완전한 제어권을 사용자에게 제공"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_teddynote.messages import stream_response\n",
    "from langchain_together import ChatTogether\n",
    "\n",
    "together = ChatTogether(model=\"meta-llama/Llama-3.3-70B-Instruct-Turbo\", temperature=0)\n",
    "# together = ChatTogether(model=\"google/gemma-2-27b-it\", temperature=0)\n",
    "\n",
    "answer = together.stream(\"로또 생성기 파이썬 코드를 작성하세요\")\n",
    "stream_response(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "function calling 지원"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class Lotto(BaseModel):\n",
    "    numbers: List[int]\n",
    "\n",
    "\n",
    "together_structured = together.with_structured_output(Lotto)\n",
    "\n",
    "answer = together_structured.invoke(\"로또 번호 6개를 추천해 주세요\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cohere\n",
    "\n",
    "Cohere는 기업용 인공지능 솔루션을 제공하는 선도적인 AI 기업으로, 대규모 언어 모델(LLM)을 개발하여 기업들이 AI 기술을 쉽게 도입하고 활용할 수 있도록 돕고 있습니다.\n",
    "\n",
    "## Cohere 개요\n",
    "\n",
    "- **설립연도**: 2020년\n",
    "- **주요 투자자**: Inovia Capital, NVIDIA, Oracle, Salesforce Ventures\n",
    "- **시리즈 C 펀딩**: 2억 7000만 달러 유치\n",
    "- **기업 미션**: 기업용 AI 플랫폼 제공\n",
    "\n",
    "## 주요 제품\n",
    "\n",
    "### Command R+\n",
    "\n",
    "Command R+는 기업용으로 최적화된 Cohere의 최신 LLM입니다. \n",
    "\n",
    "#### 주요 특징\n",
    "\n",
    "- **긴 컨텍스트 윈도우**: 128k 토큰 지원\n",
    "- **고급 RAG 기능**: 검색 강화 생성 기능 제공\n",
    "- **다국어 지원**: 10개 주요 비즈니스 언어 지원\n",
    "- **자동화 도구 사용 기능**: 복잡한 비즈니스 프로세스 자동화\n",
    "\n",
    "### Aya\n",
    "\n",
    "Aya는 Cohere의 비영리 연구소인 Cohere for AI에서 개발한 오픈소스 다국어 LLM입니다. \n",
    "\n",
    "#### 주요 특징\n",
    "\n",
    "- **언어 지원**: 101개 언어 지원 (기존 오픈소스 모델의 두 배 이상)\n",
    "- **훈련 데이터셋**: 5억 1300만 개의 데이터 포인트 포함하는 대규모 다국어 훈련 데이터셋 공개\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_cohere import ChatCohere\n",
    "\n",
    "# ChatCohere 객체를 생성합니다.\n",
    "cohere = ChatCohere(temperature=0)\n",
    "\n",
    "# 스트리밍 출력을 위하여 invoke() 대신 stream()을 사용합니다.\n",
    "answer = cohere.stream(\"사랑이 뭔가요?\")\n",
    "\n",
    "# 답변 출력\n",
    "stream_response(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upstage\n",
    "\n",
    "Upstage는 인공지능(AI) 기술, 특히 대규모 언어 모델(LLM)과 문서 AI 분야에 특화된 국내 스타트업입니다.\n",
    "\n",
    "## 주요 제품 및 기술\n",
    "\n",
    "### Solar LLM\n",
    "- **주요 특징**: Upstage의 주력 대규모 언어 모델로, 빠른 성능과 비용 효율성으로 주목받고 있습니다.\n",
    "- **기술적 접근**: Depth-Up Scaling (DUS) 기술을 적용하여 성능을 극대화합니다.\n",
    "- **플랫폼 통합**: Amazon SageMaker JumpStart 등 다양한 플랫폼을 통해 API로 통합 제공됩니다.\n",
    "\n",
    "### Document AI Pack\n",
    "- **기능**: OCR 기술을 기반으로 한 문서 처리 솔루션으로, 복잡한 문서에서 필요한 내용을 정확히 추출하고 디지털화합니다.\n",
    "\n",
    "### AskUp Seargest\n",
    "- **특징**: 개인화된 검색 및 추천 서비스를 제공하며, 기존의 ChatGPT 통합 무료 챗봇 AskUp의 업그레이드 버전입니다.\n",
    "\n",
    "## API 키 발급\n",
    "API 키 발급은 [여기](https://console.upstage.ai/api-keys)에서 가능합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UPSTAGE API KEY 설정\n",
    "# import os\n",
    "\n",
    "# os.environ[\"UPSTAGE_API_KEY\"] = \"이곳에 API KEY를 입력하세요.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_upstage import ChatUpstage\n",
    "\n",
    "# ChatUpstage 객체를 생성합니다.\n",
    "upstage = ChatUpstage(model=\"solar-pro\")\n",
    "\n",
    "# 스트리밍 출력을 위하여 invoke() 대신 stream()을 사용합니다.\n",
    "answer = upstage.stream(\"사랑이 뭔가요?\")\n",
    "\n",
    "# 답변 출력\n",
    "stream_response(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Xionic\n",
    "\n",
    "사이오닉에이아이(Sionic AI)는 대한민국의 유망한 인공지능 스타트업으로, 기업용 생성형 AI 솔루션을 개발하고 있습니다. 다음은 이 회사에 대한 주요 정보입니다:\n",
    "\n",
    "### 주요 제품\n",
    "\n",
    "1. **STORM Platform**: 기업이 생성형 AI를 기술적 고민 없이 바로 적용할 수 있도록 하는 플랫폼\n",
    "2. **STORM Answer**: 기업에 최적화된 생성형 AI 솔루션으로 비즈니스 생산성 향상을 목표로 함\n",
    "3. **Xionic**: 상업적 이용이 가능한 라이센스의 한국어 AI 모델\n",
    "\n",
    "상업적 이용이 가능한 라이센스의 한국어 모델\n",
    "\n",
    "- 링크: https://github.com/sionic-ai/xionic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(참고) 2024.11.29 기준 사용불가 (API 서버 오류)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_teddynote.messages import stream_response\n",
    "\n",
    "# 2024. 11. 21 업데이트 코드\n",
    "xionic = ChatOpenAI(\n",
    "    model_name=\"xionic-1-72b-20240919\",\n",
    "    base_url=\"https://sionic.chat/v1/\",\n",
    "    api_key=\"934c4bbc-c384-4bea-af82-1450d7f8128d\",\n",
    ")\n",
    "\n",
    "# 스트리밍 출력을 위하여 invoke() 대신 stream()을 사용합니다.\n",
    "answer = xionic.stream(\"사랑이 뭔가요?\")\n",
    "\n",
    "# 답변 출력\n",
    "stream_response(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LogicKor\n",
    "\n",
    "LogicKor는 한국어 언어 모델의 다분야 사고력을 평가하기 위해 만들어진 벤치마크 리더보드입니다.\n",
    "\n",
    "## 목적\n",
    "\n",
    "한국어 언어 모델의 다양한 분야에서의 사고력을 측정하는 벤치마크\n",
    "\n",
    "## 평가 영역\n",
    "\n",
    "- 한국어 추론\n",
    "- 수학\n",
    "- 글쓰기\n",
    "- 코딩\n",
    "- 이해력\n",
    "\n",
    "## 주요 특징\n",
    "\n",
    "1. **다양한 모델 평가**: 국내외 다양한 언어 모델들의 성능을 비교할 수 있음\n",
    "2. **객관적 성능 측정**: 모델의 실제 성능을 다각도로 평가하여 객관적인 지표 제공\n",
    "3. **오픈 소스**: 누구나 접근하고 결과를 확인할 수 있는 오픈 플랫폼\n",
    "\n",
    "LogicKor 리더보드는 한국어 AI 모델의 발전을 위한 중요한 도구로 자리잡고 있으며, 지속적인 개선과 발전이 기대되고 있습니다.\n",
    "\n",
    "- 링크: [LogicKor 리더보드](https://lk.instruct.kr/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 캐싱(Caching)\n",
    "\n",
    "LangChain은 LLM을 위한 선택적 캐싱 레이어를 제공합니다.\n",
    "\n",
    "이는 두 가지 이유로 유용합니다.\n",
    "\n",
    "- 동일한 완료를 여러 번 요청하는 경우 LLM 공급자에 대한 **API 호출 횟수를 줄여 비용을 절감**할 수 있습니다.\n",
    "- LLM 제공업체에 대한 **API 호출 횟수를 줄여 애플리케이션의 속도를 높일 수** 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API KEY를 환경변수로 관리하기 위한 설정 파일\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# API KEY 정보로드\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangSmith 추적을 설정합니다. https://smith.langchain.com\n",
    "# !pip install langchain-teddynote\n",
    "from langchain_teddynote import logging\n",
    "\n",
    "# 프로젝트 이름을 입력합니다.\n",
    "logging.langsmith(\"CH04-Models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델과 프롬프트를 생성합니다\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# 모델을 생성합니다.\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\")\n",
    "\n",
    "# 프롬프트를 생성합니다.\n",
    "prompt = PromptTemplate.from_template(\"{country} 에 대해서 200자 내외로 요약해줘\")\n",
    "\n",
    "# 체인을 생성합니다.\n",
    "chain = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "response = chain.invoke({\"country\": \"한국\"})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## InMemoryCache\n",
    "\n",
    "인메모리 캐시를 사용하여 동일 질문에 대한 답변을 저장하고, 캐시에 저장된 답변을 반환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from langchain.globals import set_llm_cache\n",
    "from langchain.cache import InMemoryCache\n",
    "\n",
    "# 인메모리 캐시를 사용합니다.\n",
    "set_llm_cache(InMemoryCache())\n",
    "\n",
    "# 체인을 실행합니다.\n",
    "response = chain.invoke({\"country\": \"한국\"})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# 체인을 실행합니다.\n",
    "response = chain.invoke({\"country\": \"한국\"})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQLite Cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.cache import SQLiteCache\n",
    "from langchain_core.globals import set_llm_cache\n",
    "import os\n",
    "\n",
    "# 캐시 디렉토리를 생성합니다.\n",
    "if not os.path.exists(\"cache\"):\n",
    "    os.makedirs(\"cache\")\n",
    "\n",
    "# SQLiteCache를 사용합니다.\n",
    "set_llm_cache(SQLiteCache(database_path=\"cache/llm_cache.db\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "# 체인을 실행합니다.\n",
    "response = chain.invoke({\"country\": \"한국\"})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 직렬화(Serialization)\n",
    "\n",
    "### 직렬화(Serialization) 란?\n",
    "\n",
    "1. **정의:**\n",
    "   - 모델을 저장 가능한 형식으로 변환하는 과정\n",
    "\n",
    "2. **목적:**\n",
    "   - 모델 재사용 (재훈련 없이)\n",
    "   - 모델 배포 및 공유 용이\n",
    "   - 계산 리소스 절약\n",
    "\n",
    "3. **장점:**\n",
    "   - 빠른 모델 로딩\n",
    "   - 버전 관리 가능\n",
    "   - 다양한 환경에서 사용 가능\n",
    "\n",
    "모델 직렬화는 AI 개발 및 배포 과정에서 중요한 단계로, 효율적인 모델 관리와 재사용을 가능하게 합니다.\n",
    "\n",
    "`is_lc_serializable` 클래스 메서드로 실행하여 LangChain 클래스가 직렬화 가능한지 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API KEY를 환경변수로 관리하기 위한 설정 파일\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# API KEY 정보로드\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangSmith 추적을 설정합니다. https://smith.langchain.com\n",
    "# !pip install langchain-teddynote\n",
    "from langchain_teddynote import logging\n",
    "\n",
    "# 프로젝트 이름을 입력합니다.\n",
    "logging.langsmith(\"CH04-Models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# 프롬프트 템플릿을 사용하여 질문을 생성합니다.\n",
    "prompt = PromptTemplate.from_template(\"{fruit}의 색상이 무엇입니까?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "클래스(class) 에 대하여 직렬화 가능 여부를 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 직렬화가 가능한지 체크합니다.\n",
    "print(f\"ChatOpenAI: {ChatOpenAI.is_lc_serializable()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "llm 객체에 대하여 직렬화 가능 여부를 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "# 직렬화가 가능한지 체크합니다.\n",
    "print(f\"ChatOpenAI: {llm.is_lc_serializable()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 체인을 생성합니다.\n",
    "chain = prompt | llm\n",
    "\n",
    "# 직렬화가 가능한지 체크합니다.\n",
    "chain.is_lc_serializable()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 체인(Chain) 직렬화(dumps, dumpd)\n",
    "\n",
    "### 개요\n",
    "\n",
    "체인 직렬화는 직렬화 가능한 모든 객체를 딕셔너리 또는 JSON 문자열로 변환하는 과정을 의미합니다.\n",
    "\n",
    "### 직렬화 방법\n",
    "\n",
    "객체의 속성 및 데이터를 키-값 쌍으로 저장하여 딕셔너리 형태로 변환합니다.\n",
    "\n",
    "이러한 직렬화 방식은 객체를 쉽게 저장하고 전송할 수 있게 하며, 다양한 환경에서 객체를 재구성할 수 있도록 합니다.\n",
    "\n",
    "**참고**\n",
    "- `dumps`: 객체를 JSON 문자열로 직렬화\n",
    "- `dumpd`: 객체를 딕셔너리로 직렬화\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.load import dumpd, dumps\n",
    "\n",
    "dumpd_chain = dumpd(chain)\n",
    "dumpd_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 직렬화된 체인의 타입을 확인합니다.\n",
    "type(dumpd_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번에는 `dumps` 함수를 사용하여 직렬화된 체인을 확인해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dumps 함수를 사용하여 직렬화된 체인을 확인합니다.\n",
    "dumps_chain = dumps(chain)\n",
    "dumps_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 직렬화된 체인의 타입을 확인합니다.\n",
    "type(dumps_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pickle 파일\n",
    "\n",
    "### 개요\n",
    "\n",
    "Pickle 파일은 Python 객체를 바이너리 형태로 직렬화하는 포맷입니다.\n",
    "\n",
    "### 특징\n",
    "\n",
    "1. **형식:**\n",
    "   - Python 객체를 바이너리 형태로 직렬화하는 포맷\n",
    "\n",
    "2. **특징:**\n",
    "   - Python 전용 (다른 언어와 호환 불가)\n",
    "   - 대부분의 Python 데이터 타입 지원 (리스트, 딕셔너리, 클래스 등)\n",
    "   - 객체의 상태와 구조를 그대로 보존\n",
    "\n",
    "3. **장점:**\n",
    "   - 효율적인 저장 및 전송\n",
    "   - 복잡한 객체 구조 유지\n",
    "   - 빠른 직렬화/역직렬화 속도\n",
    "\n",
    "4. **단점:**\n",
    "   - 보안 위험 (신뢰할 수 없는 데이터 역직렬화 시 주의 필요)\n",
    "   - 사람이 읽을 수 없는 바이너리 형식\n",
    "\n",
    "### 주요 용도\n",
    "\n",
    "1. 객체 캐싱\n",
    "2. 머신러닝 모델 저장\n",
    "3. 프로그램 상태 저장 및 복원\n",
    "\n",
    "### 사용법\n",
    "\n",
    "- `pickle.dump()`: 객체를 파일에 저장\n",
    "- `pickle.load()`: 파일에서 객체 로드\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pickle 파일로 저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# fuit_chain.pkl 파일로 직렬화된 체인을 저장합니다.\n",
    "with open(\"fruit_chain.pkl\", \"wb\") as f:\n",
    "    pickle.dump(dumpd_chain, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JSON 형식으로 마찬가지로 저장할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"fruit_chain.json\", \"w\") as fp:\n",
    "    json.dump(dumpd_chain, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load: 저장한 모델 불러오기\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "먼저, 이전에 저장한 `pickle` 형식의 파일을 로드합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# pickle 파일을 로드합니다.\n",
    "with open(\"fruit_chain.pkl\", \"rb\") as f:\n",
    "    loaded_chain = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "로드한 json 파일을 `load` 메서드를 사용하여 로드합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.load import load\n",
    "\n",
    "# 체인을 로드합니다.\n",
    "chain_from_file = load(loaded_chain)\n",
    "\n",
    "# 체인을 실행합니다.\n",
    "print(chain_from_file.invoke({\"fruit\": \"사과\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.load import load, loads\n",
    "\n",
    "load_chain = load(\n",
    "    loaded_chain, secrets_map={\"OPENAI_API_KEY\": os.environ[\"OPENAI_API_KEY\"]}\n",
    ")\n",
    "\n",
    "# 불러온 체인이 정상 동작하는지 확인합니다.\n",
    "load_chain.invoke({\"fruit\": \"사과\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"fruit_chain.json\", \"r\") as fp:\n",
    "    loaded_from_json_chain = json.load(fp)\n",
    "    loads_chain = load(loaded_from_json_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불러온 체인이 정상 동작하는지 확인합니다.\n",
    "loads_chain.invoke({\"fruit\": \"사과\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 토큰 사용량 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API KEY를 환경변수로 관리하기 위한 설정 파일\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# API KEY 정보로드\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangSmith 추적을 설정합니다. https://smith.langchain.com\n",
    "# !pip install langchain-teddynote\n",
    "from langchain_teddynote import logging\n",
    "\n",
    "# 프로젝트 이름을 입력합니다.\n",
    "logging.langsmith(\"CH04-Models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "특정 호출에 대한 토큰 사용량을 추적하는 방법에 대해 설명합니다.\n",
    "\n",
    "이 기능은 현재 OpenAI API 에만 구현되어 있습니다.\n",
    "\n",
    "먼저 단일 Chat 모델 호출에 대한 토큰 사용량을 추적하는 매우 간단한 예를 살펴보겠습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks import get_openai_callback\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# 모델을 불러옵니다.\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`with get_openai_callback()` 구문안에서 실행되는 모든 토큰 사용량/요금이 추적됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callback을 사용하여 추적합니다.\n",
    "with get_openai_callback() as cb:\n",
    "    result = llm.invoke(\"대한민국의 수도는 어디야?\")\n",
    "    print(cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callback을 사용하여 추적합니다.\n",
    "with get_openai_callback() as cb:\n",
    "    result = llm.invoke(\"대한민국의 수도는 어디야?\")\n",
    "    result = llm.invoke(\"대한민국의 수도는 어디야?\")\n",
    "    print(f\"총 사용된 토큰수: \\t\\t{cb.total_tokens}\")\n",
    "    print(f\"프롬프트에 사용된 토큰수: \\t{cb.prompt_tokens}\")\n",
    "    print(f\"답변에 사용된 토큰수: \\t{cb.completion_tokens}\")\n",
    "    print(f\"호출에 청구된 금액(USD): \\t${cb.total_cost}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google AI chat models (gemini-pro)\n",
    "\n",
    "Google AI의 `gemini`와 `gemini-vision` 모델뿐만 아니라 다른 생성 모델에 접근하려면 [langchain-google-genai](https://pypi.org/project/langchain-google-genai/) 통합 패키지의 `ChatGoogleGenerativeAI` 클래스를 사용하면 됩니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -qU langchain-google-genai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API KEY 발급받기\n",
    "\n",
    "- [링크](https://makersuite.google.com/app/apikey?hl=ko) 에서 API KEY를 발급받아주세요.\n",
    "- 사용자의 Google API 키를 환경 변수 `GOOGLE_API_KEY`로 설정합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangSmith 추적을 설정합니다. https://smith.langchain.com\n",
    "# !pip install langchain-teddynote\n",
    "from langchain_teddynote import logging\n",
    "from langchain_teddynote.messages import stream_response\n",
    "\n",
    "# 프로젝트 이름을 입력합니다.\n",
    "logging.langsmith(\"CH04-Models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "langchain_google_genai 패키지에서 ChatGoogleGenerativeAI 클래스를 가져옵니다.\n",
    "\n",
    "- ChatGoogleGenerativeAI 클래스는 Google의 Generative AI 모델을 사용하여 대화형 AI 시스템을 구현하는 데 사용됩니다.\n",
    "- 이 클래스를 통해 사용자는 Google의 대화형 AI 모델과 상호 작용할 수 있습니다.\n",
    "- 모델과의 대화는 채팅 형식으로 이루어지며, 사용자의 입력에 따라 모델이 적절한 응답을 생성합니다.\n",
    "- ChatGoogleGenerativeAI 클래스는 LangChain 프레임워크와 통합되어 있어, 다른 LangChain 컴포넌트와 함께 사용할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "지원되는 모델 정보: https://ai.google.dev/gemini-api/docs/models/gemini?hl=ko"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# ChatGoogleGenerativeAI 언어 모델을 초기화합니다.\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro-latest\")\n",
    "\n",
    "# 프롬프트를 전달하여 결과를 생성합니다.\n",
    "answer = llm.stream(\"자연어처리에 대해서 간략히 설명해 줘\")\n",
    "\n",
    "# 결과를 출력합니다.\n",
    "stream_response(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# ChatGoogleGenerativeAI 언어 모델을 초기화합니다.\n",
    "model = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash-latest\",  # 사용할 모델을 지정합니다.\n",
    ")\n",
    "\n",
    "# 프롬프트를 생성합니다.\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"예/아니오 질문에 대답하세요. {question}는 과일입니까?\"\n",
    ")\n",
    "\n",
    "# 체인을 생성합니다.\n",
    "chain = prompt | model\n",
    "\n",
    "# 결과를 출력합니다.\n",
    "stream_response(chain.stream({\"question\": \"사과\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Safety Settings\n",
    "\n",
    "Gemini 모델에는 기본 안전 설정(Satety Settings) 이 있지만, 이를 재정의할 수 있습니다.\n",
    "\n",
    "만약 모델로부터 많은 \"Safety Warnings\"를 받고 있다면, 모델의 `safety_settings` 속성을 조정해 볼 수 있습니다.\n",
    "\n",
    "Google의 [Safety Setting Types](https://ai.google.dev/api/python/google/generativeai/types/SafetySettingDict) 문서에서는 사용 가능한 카테고리와 임계값에 대한 열거형 정보를 제공합니다.\n",
    "\n",
    "이 문서에는 콘텐츠 필터링 및 안전 설정과 관련된 다양한 카테고리와 해당 임계값이 정의되어 있어, 개발자들이 생성형 AI 모델을 활용할 때 적절한 안전 설정을 선택하고 적용하는 데 도움을 줍니다.\n",
    "\n",
    "이를 통해 개발자들은 모델이 생성하는 콘텐츠의 안전성과 적절성을 보장하고, 사용자에게 유해하거나 부적절한 내용이 노출되는 것을 방지할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import (\n",
    "    ChatGoogleGenerativeAI,\n",
    "    HarmBlockThreshold,\n",
    "    HarmCategory,\n",
    ")\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    # 사용할 모델을 \"gemini-pro\"로 지정합니다.\n",
    "    model=\"gemini-1.5-pro-latest\",\n",
    "    safety_settings={\n",
    "        # 위험한 콘텐츠에 대한 차단 임계값을 설정합니다.\n",
    "        # 이 경우 위험한 콘텐츠를 차단하지 않도록 설정되어 있습니다. (그럼에도 기본적인 차단이 있을 수 있습니다.)\n",
    "        HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,\n",
    "        HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n",
    "        HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "        HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch 단위 실행\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    # 사용할 모델을 \"gemini-pro\"로 지정합니다.\n",
    "    model=\"gemini-1.5-pro-latest\",\n",
    ")\n",
    "\n",
    "results = llm.batch(\n",
    "    [\n",
    "        \"대한민국의 수도는?\",\n",
    "        \"대한민국의 주요 관광지 5곳을 나열하세요\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "for res in results:\n",
    "    # 각 결과의 내용을 출력합니다.\n",
    "    print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multimodal 모델\n",
    "\n",
    "`langchain-teddynote` 에서 구현한 멀티모달 모델에 `gemini-1.5-pro` 모델을 활용하여 이미지를 텍스트로 변환 가능합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_teddynote.models import MultiModal\n",
    "from langchain_teddynote.messages import stream_response\n",
    "\n",
    "# 객체 생성\n",
    "gemini = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro-latest\")\n",
    "\n",
    "system_prompt = (\n",
    "    \"당신은 시인입니다. 당신의 임무는 주어진 이미지를 가지고 시를 작성하는 것입니다.\"\n",
    ")\n",
    "\n",
    "user_prompt = \"다음의 이미지에 대한 시를 작성해주세요.\"\n",
    "\n",
    "# 멀티모달 객체 생성\n",
    "multimodal_gemini = MultiModal(\n",
    "    llm, system_prompt=system_prompt, user_prompt=user_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 샘플 이미지 경로(파일의 경로, URL 등)를 지정합니다.\n",
    "IMAGE_URL = \"images/jeju-beach.jpg\"\n",
    "\n",
    "# 이미지 파일로 부터 질의\n",
    "answer = multimodal_gemini.stream(IMAGE_URL)\n",
    "\n",
    "# 스트리밍 방식으로 각 토큰을 출력합니다. (실시간 출력)\n",
    "stream_response(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Huggingface Endpoints\n",
    "\n",
    "Hugging Face Hub은 12만 개 이상의 모델, 2만 개의 데이터셋, 5만 개의 데모 앱(Spaces)을 보유한 플랫폼으로, 모두 오픈 소스이며 공개적으로 사용 가능합니다. 이 온라인 플랫폼에서 사람들은 쉽게 협업하고 함께 머신러닝을 구축할 수 있습니다.\n",
    "\n",
    "Hugging Face Hub은 또한 다양한 ML 애플리케이션을 구축하기 위한 다양한 엔드포인트를 제공합니다. 이 예제는 다양한 유형의 엔드포인트에 연결하는 방법을 보여줍니다.\n",
    "\n",
    "특히, 텍스트 생성 추론은 Text Generation Inference에 의해 구동됩니다. 이는 매우 빠른 텍스트 생성 추론을 위해 맞춤 제작된 Rust, Python, gRPC 서버입니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 허깅페이스 토큰 발급\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "허깅페이스(https://huggingface.co) 에 회원가입을 한 뒤, 아래의 주소에서 토큰 발급을 신청합니다.\n",
    "\n",
    "- 토큰 발급주소: https://huggingface.co/docs/hub/security-tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 참고 모델 리스트\n",
    "\n",
    "- 허깅페이스 LLM 리더보드: https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard\n",
    "- 모델 리스트: https://huggingface.co/models?pipeline_tag=text-generation&sort=downloads\n",
    "- LogicKor 리더보드: https://lk.instruct.kr/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Huggingface Endpoints 사용\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "사용하기 위해서는 Python의 `huggingface_hub` [패키지를 설치](https://huggingface.co/docs/huggingface_hub/installation)해야 합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -qU huggingface_hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.env` 파일에 이미 발급받은 토큰을 `HUGGINGFACEHUB_API_TOKEN` 을 저장한 뒤 다음 단계롤 진행합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`HUGGINGFACEHUB_API_TOKEN` 을 불러옵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangSmith 추적을 설정합니다. https://smith.langchain.com\n",
    "# !pip install langchain-teddynote\n",
    "from langchain_teddynote import logging\n",
    "\n",
    "# 프로젝트 이름을 입력합니다.\n",
    "logging.langsmith(\"CH04-Models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "허깅페이스 토큰을 입력합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "간단한 프롬프트를 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "template = \"\"\"<|system|>\n",
    "You are a helpful assistant.<|end|>\n",
    "<|user|>\n",
    "{question}<|end|>\n",
    "<|assistant|>\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serverless Endpoints\n",
    "\n",
    "Inference API 는 무료로 사용할 수 있으며 요금은 제한되어 있습니다. 프로덕션을 위한 추론 솔루션이 필요한 경우, [Inference Endpoints](https://huggingface.co/docs/inference-endpoints/index) 서비스를 확인하세요. Inference Endpoints 를 사용하면 모든 머신 러닝 모델을 전용 및 완전 관리형 인프라에 손쉽게 배포할 수 있습니다. 클라우드, 지역, 컴퓨팅 인스턴스, 자동 확장 범위 및 보안 수준을 선택하여 모델, 지연 시간, 처리량 및 규정 준수 요구 사항에 맞게 설정하세요.\n",
    "\n",
    "다음은 Inference API 에 액세스하는 방법의 예시입니다.\n",
    "\n",
    "**참고**\n",
    "\n",
    "- [Serverless Endpoints](https://huggingface.co/docs/api-inference/index)\n",
    "- [Inference Endpoints](https://huggingface.co/docs/inference-endpoints/index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`repo_id` 변수에 HuggingFace 모델의 `repo ID`(저장소 ID) 를 할당합니다.\n",
    "- `microsoft/Phi-3-mini-4k-instruct` 모델: https://huggingface.co/microsoft/Phi-3-mini-4k-instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "\n",
    "# 사용할 모델의 저장소 ID를 설정합니다.\n",
    "repo_id = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=repo_id,  # 모델 저장소 ID를 지정합니다.\n",
    "    max_new_tokens=256,  # 생성할 최대 토큰 길이를 설정합니다.\n",
    "    temperature=0.1,\n",
    "    huggingfacehub_api_token=os.environ[\"HUGGINGFACEHUB_API_TOKEN\"],  # 허깅페이스 토큰\n",
    ")\n",
    "\n",
    "# LLMChain을 초기화하고 프롬프트와 언어 모델을 전달합니다.\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "# 질문을 전달하여 LLMChain을 실행하고 결과를 출력합니다.\n",
    "response = chain.invoke({\"question\": \"what is the capital of South Korea?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 전용 엔드포인트(Dedicated Endpoint)\n",
    "\n",
    "무료 서버리스 API를 사용하면 솔루션을 빠르게 구현하고 반복할 수 있습니다. 하지만 로드가 다른 요청과 공유되기 때문에 대용량 사용 사례에서는 속도 제한이 있을 수 있습니다.\n",
    "\n",
    "엔터프라이즈 워크로드의 경우, [Inference Endpoints - Dedicated](https://huggingface.co/inference-endpoints/dedicated)를 사용하는 것이 가장 좋습니다. 이를 통해 더 많은 유연성과 속도를 제공하는 완전 관리형 인프라에 액세스할 수 있습니다.\n",
    "\n",
    "이러한 리소스에는 지속적인 지원과 가동 시간 보장은 물론 AutoScaling과 같은 옵션도 포함됩니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/hf-endpoint-1.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `hf_endpoint_url` 변수에 Inference Endpoint의 URL을 설정합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference Endpoint URL을 아래에 설정합니다.\n",
    "hf_endpoint_url = \"https://slcalzucia3n7y3g.us-east-1.aws.endpoints.huggingface.cloud\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = HuggingFaceEndpoint(\n",
    "    # 엔드포인트 URL을 설정합니다.\n",
    "    endpoint_url=hf_endpoint_url,\n",
    "    max_new_tokens=512,\n",
    "    temperature=0.01,\n",
    ")\n",
    "\n",
    "# 주어진 프롬프트에 대해 언어 모델을 실행합니다.\n",
    "llm.invoke(input=\"대한민국의 수도는 어디인가요?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"A chat between a curious user and an artificial intelligence assistant. \"\n",
    "            \"The assistant gives helpful, detailed, and polite answers to the user's questions.\",\n",
    "        ),\n",
    "        (\"user\", \"Human: {question}\\nAssistant: \"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke(\"대한민국의 수도는?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰 정보로드를 위한 라이브러리\n",
    "# 설치: pip install python-dotenv\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# 토큰 정보로드\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangSmith 추적을 설정합니다. https://smith.langchain.com\n",
    "# !pip install langchain-teddynote\n",
    "from langchain_teddynote import logging\n",
    "\n",
    "# 프로젝트 이름을 입력합니다.\n",
    "logging.langsmith(\"CH04-Models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델을 다운로드 받을 경로를 설정\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 허깅페이스 모델/토크나이저를 다운로드 받을 경로\n",
    "import os\n",
    "\n",
    "# ./cache/ 경로에 다운로드 받도록 설정\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"./cache/\"\n",
    "os.environ[\"HF_HOME\"] = \"./cache/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`repo_id` 변수에 HuggingFace 모델의 `repo ID`(저장소 ID) 를 할당합니다.\n",
    "- `microsoft/Phi-3-mini-4k-instruct` 모델: https://huggingface.co/microsoft/Phi-3-mini-4k-instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFacePipeline\n",
    "\n",
    "llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"microsoft/Phi-3-mini-4k-instruct\",\n",
    "    task=\"text-generation\",\n",
    "    pipeline_kwargs={\n",
    "        \"max_new_tokens\": 256,\n",
    "        \"top_k\": 50,\n",
    "        \"temperature\": 0.1,\n",
    "    },\n",
    ")\n",
    "llm.invoke(\"Hugging Face is\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"Summarizes TEXT in simple bullet points ordered from most important to least important.\n",
    "TEXT:\n",
    "{text}\n",
    "\n",
    "KeyPoints: \"\"\"\n",
    "\n",
    "# 프롬프트 템플릿 생성\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "# 체인 생성\n",
    "chain = prompt | llm\n",
    "\n",
    "text = \"\"\"A Large Language Model (LLM) like me, ChatGPT, is a type of artificial intelligence (AI) model designed to understand, generate, and interact with human language. These models are \"large\" because they're built from vast amounts of text data and have billions or even trillions of parameters. Parameters are the aspects of the model that are learned from training data; they are essentially the internal settings that determine how the model interprets and generates language. LLMs work by predicting the next word in a sequence given the words that precede it, which allows them to generate coherent and contextually relevant text based on a given prompt. This capability can be applied in a variety of ways, from answering questions and composing emails to writing essays and even creating computer code. The training process for these models involves exposing them to a diverse array of text sources, such as books, articles, and websites, allowing them to learn language patterns, grammar, facts about the world, and even styles of writing. However, it's important to note that while LLMs can provide information that seems knowledgeable, their responses are generated based on patterns in the data they were trained on and not from a sentient understanding or awareness. The development and deployment of LLMs raise important considerations regarding accuracy, bias, ethical use, and the potential impact on various aspects of society, including employment, privacy, and misinformation. Researchers and developers continue to work on ways to address these challenges while improving the models' capabilities and applications.\"\"\"\n",
    "print(f\"입력 텍스트:\\n\\n{text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chain 실행\n",
    "response = chain.invoke({\"text\": text})\n",
    "\n",
    "# 결과 출력\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hugging Face Local Pipelines\n",
    "\n",
    "`HuggingFacePipeline` 클래스를 통해 Hugging Face 모델을 로컬에서 실행할 수 있습니다.\n",
    "\n",
    "[Hugging Face Model Hub](https://huggingface.co/models)는 온라인 플랫폼에서 120,000개 이상의 모델, 20,000개의 데이터셋, 50,000개의 데모 앱(Spaces)을 호스팅하며, 모두 오픈 소스이고 공개적으로 사용 가능하여 사람들이 쉽게 협업하고 함께 ML을 구축할 수 있습니다.\n",
    "\n",
    "이러한 모델은 LangChain에서 이 로컬 파이프라인 래퍼를 통해 호출하거나, HuggingFaceHub 클래스를 통해 호스팅된 추론 엔드포인트를 호출하여 사용할 수 있습니다. 호스팅된 파이프라인에 대한 자세한 내용은 [HuggingFaceHub](./huggingface_hub) 노트북을 참조하세요.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "사용하기 위해서는 [PyTorch](https://pytorch.org/get-started/locally/)와 함께 Python [패키지 transformers](https://pypi.org/project/transformers/)가 설치되어 있어야 합니다.\n",
    "\n",
    "또한, 보다 메모리 효율적인 attention 구현을 위해 `xformer`를 설치할 수도 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 설치\n",
    "# !pip install -qU transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델을 다운로드 받을 경로를 설정\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 허깅페이스 모델/토크나이저를 다운로드 받을 경로\n",
    "import os\n",
    "\n",
    "# ./cache/ 경로에 다운로드 받도록 설정\n",
    "os.environ[\"HF_HOME\"] = \"./cache/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangSmith 추적을 설정합니다. https://smith.langchain.com\n",
    "# !pip install langchain-teddynote\n",
    "from langchain_teddynote import logging\n",
    "from langchain_teddynote.messages import stream_response\n",
    "\n",
    "# 프로젝트 이름을 입력합니다.\n",
    "logging.langsmith(\"CH04-Models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Loading\n",
    "\n",
    "모델은 `from_model_id` 메서드를 사용하여 모델 매개변수를 지정함으로써 로드할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `HuggingFacePipeline` 클래스를 사용하여 Hugging Face의 사전 학습된 모델을 로드합니다.\n",
    "- `from_model_id` 메서드를 사용하여 `beomi/llama-2-ko-7b` 모델을 지정하고, 작업을 \"text-generation\"으로 설정합니다.\n",
    "- `pipeline_kwargs` 매개변수를 사용하여 생성할 최대 토큰 수를 10으로 제한합니다.\n",
    "- 로드된 모델은 `hf` 변수에 할당되며, 이를 통해 텍스트 생성 작업을 수행할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "사용한 모델: https://huggingface.co/beomi/llama-2-ko-7b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFacePipeline\n",
    "\n",
    "# HuggingFace 모델을 다운로드 받습니다.\n",
    "hf = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"beomi/llama-2-ko-7b\",  # 사용할 모델의 ID를 지정합니다.\n",
    "    task=\"text-generation\",  # 수행할 작업을 지정합니다. 여기서는 텍스트 생성입니다.\n",
    "    # 파이프라인에 전달할 추가 인자를 설정합니다. 여기서는 생성할 최대 토큰 수를 10으로 제한합니다.\n",
    "    pipeline_kwargs={\"max_new_tokens\": 512},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기존의 `transformers` pipeline을 직접 전달하여 로드할 수도 있습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HuggingFacePipeline을 사용하여 텍스트 생성 모델을 구현합니다.\n",
    "\n",
    "- `AutoTokenizer`와 `AutoModelForCausalLM`을 사용하여 `beomi/llama-2-ko-7b` 모델과 토크나이저를 로드합니다.\n",
    "- `pipeline` 함수를 사용하여 \"text-generation\" 파이프라인을 생성하고, 모델과 토크나이저를 설정합니다. 최대 생성 토큰 수는 10으로 제한합니다.\n",
    "- `HuggingFacePipeline` 클래스를 사용하여 `hf` 객체를 생성하고, 생성된 파이프라인을 전달합니다.\n",
    "\n",
    "이렇게 생성된 `hf` 객체를 사용하여 주어진 프롬프트에 대한 텍스트 생성을 수행할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "# huggingface repository에 등록된 모델 ID를 지정합니다.\n",
    "model_id = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "\n",
    "# 토크나이저를 로드합니다.\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# 지정된 모델을 로드합니다.\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    # load_in_4bit=True, # bitsandbytes 설치되어 있는 경우(linux)\n",
    "    # attn_implementation=\"flash_attention_2\", # ampere GPU 가 있는 경우\n",
    ")\n",
    "\n",
    "# 파이프라인을 생성합니다.\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=512)\n",
    "\n",
    "# HuggingFacePipeline 객체를 생성하고, 생성된 파이프라인을 전달합니다.\n",
    "hf = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gated Model 의 사용\n",
    "\n",
    "Gated Model 은 HuggingFace 에서 라이센스 동의 하에 이용할 수 있는 모델입니다.\n",
    "\n",
    "사전에 모델 페이지에 접속하여 동의하신 후, Hugging Face 토큰을 발급 받아야 합니다.\n",
    "\n",
    "아래는 Gated 모델을 사용하기 위한 예제입니다. HuggingFace 에서 발급받은 토큰을 아래와 같이 지정해 주어야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "# huggingface repository에 등록된 모델 ID를 지정합니다.\n",
    "model_id = \"google/gemma-7b-it\"\n",
    "\n",
    "# 이곳에 발급 받은 Hugging Face 토큰을 입력합니다.\n",
    "your_huggingface_token = \"hf_............................\"\n",
    "\n",
    "# 토크나이저를 로드합니다.\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, token=your_huggingface_token)\n",
    "\n",
    "# 지정된 모델을 로드합니다.\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    token=your_huggingface_token,\n",
    "    # load_in_4bit=True, # bitsandbytes 설치되어 있는 경우(linux)\n",
    "    # attn_implementation=\"flash_attention_2\", # ampere GPU 가 있는 경우\n",
    ")\n",
    "\n",
    "# 파이프라인을 생성합니다.\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=512)\n",
    "\n",
    "# HuggingFacePipeline 객체를 생성하고, 생성된 파이프라인을 전달합니다.\n",
    "hf_llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "실행 및 결과 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in hf_llm.stream(\"대한민국의 수도는 어디야?\"):\n",
    "    print(token, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Chain\n",
    "\n",
    "모델이 메모리에 로드되면 프롬프트와 함께 구성하여 체인을 형성할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `PromptTemplate` 클래스를 사용하여 질문과 답변 형식을 정의하는 프롬프트 템플릿을 생성합니다.\n",
    "- `prompt` 객체와 `hf` 객체를 파이프라인으로 연결하여 `chain` 객체를 생성합니다.\n",
    "- `chain.invoke()` 메서드를 호출하여 주어진 질문에 대한 답변을 생성하고 출력합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import ChatHuggingFace\n",
    "\n",
    "llm = ChatHuggingFace(llm=hf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "template = \"\"\"<|system|>You are a helpful assistant.<|end|>\n",
    "<|user|>{question}<|end|>\n",
    "<|assistant|>\"\"\"  # 질문과 답변 형식을 정의하는 템플릿\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)  # 템플릿을 사용하여 프롬프트 객체 생성\n",
    "\n",
    "# 프롬프트와 언어 모델을 연결하여 체인 생성\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "question = \"대한민국의 수도는 어디야?\"  # 질문 정의\n",
    "\n",
    "# print(\n",
    "# chain.invoke({\"question\": question})\n",
    "# )  # 체인을 호출하여 질문에 대한 답변 생성 및 출력\n",
    "\n",
    "answer = chain.stream({\"question\": question})\n",
    "stream_response(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU Inference\n",
    "\n",
    "GPU에서 실행할 때는 `device=n` 매개변수를 지정하여 모델을 특정 디바이스에 배치할 수 있습니다.\n",
    "\n",
    "기본값은 `-1`로, CPU에서 추론을 수행합니다.\n",
    "\n",
    "여러 개의 GPU가 있거나 모델이 단일 GPU에 비해 너무 큰 경우에는 `device_map=\"auto\"`를 지정할 수 있습니다.\n",
    "\n",
    "이 경우 [Accelerate](https://huggingface.co/docs/accelerate/index) 라이브러리가 필요하며, 모델 가중치를 어떻게 로드할지 자동으로 결정하는 데 사용됩니다.\n",
    "\n",
    "_주의_: `device`와 `device_map`은 함께 지정해서는 안 되며, 예기치 않은 동작을 유발할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `HuggingFacePipeline`을 사용하여 `gpt2` 모델을 로드하고, `device` 매개변수를 0으로 설정하여 GPU에서 실행되도록 합니다.\n",
    "- `pipeline_kwargs` 매개변수를 사용하여 생성할 최대 토큰 수를 10으로 제한합니다.\n",
    "- `prompt`와 `gpu_llm`을 파이프라인으로 연결하여 `gpu_chain`을 생성합니다.\n",
    "- `gpu_chain.invoke()` 메서드를 호출하여 주어진 질문에 대한 답변을 생성하고 출력합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"beomi/llama-2-ko-7b\",  # 사용할 모델의 ID를 지정합니다.\n",
    "    task=\"text-generation\",  # 수행할 작업을 설정합니다. 여기서는 텍스트 생성입니다.\n",
    "    # 사용할 GPU 디바이스 번호를 지정합니다. \"auto\"로 설정하면 accelerate 라이브러리를 사용합니다.\n",
    "    device=0,\n",
    "    # 파이프라인에 전달할 추가 인자를 설정합니다. 여기서는 생성할 최대 토큰 수를 10으로 제한합니다.\n",
    "    pipeline_kwargs={\"max_new_tokens\": 64},\n",
    ")\n",
    "\n",
    "gpu_chain = prompt | gpu_llm  # prompt와 gpu_llm을 연결하여 gpu_chain을 생성합니다.\n",
    "\n",
    "# 프롬프트와 언어 모델을 연결하여 체인 생성\n",
    "gpu_chain = prompt | gpu_llm | StrOutputParser()\n",
    "\n",
    "question = \"대한민국의 수도는 어디야?\"  # 질문 정의\n",
    "\n",
    "# 체인을 호출하여 질문에 대한 답변 생성 및 출력\n",
    "print(gpu_chain.invoke({\"question\": question}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch GPU Inference\n",
    "\n",
    "GPU 장치에서 실행하는 경우, 배치 모드로 GPU에서 추론을 실행할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `HuggingFacePipeline`을 사용하여 `beomi/llama-2-ko-7b` 모델을 로드하고, GPU에서 실행되도록 설정합니다.\n",
    "- `gpu_llm`을 생성할 때 `batch_size`를 2로 설정하고, `temperature`를 0으로, `max_length`를 64로 설정합니다.\n",
    "- `prompt`와 `gpu_llm`을 파이프라인으로 연결하여 `gpu_chain`을 생성하고, 종료 토큰을 \"\\n\\n\"로 설정합니다.\n",
    "- `gpu_chain.batch()`를 사용하여 `questions`의 질문들에 대한 답변을 병렬로 생성합니다.\n",
    "- 생성된 답변을 반복문을 통해 출력합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"beomi/llama-2-ko-7b\",  # 사용할 모델의 ID를 지정합니다.\n",
    "    task=\"text-generation\",  # 수행할 작업을 설정합니다.\n",
    "    device=0,  # GPU 디바이스 번호를 지정합니다. -1은 CPU를 의미합니다.\n",
    "    batch_size=2,  # 배치 크기s를 조정합니다. GPU 메모리와 모델 크기에 따라 적절히 설정합니다.\n",
    "    model_kwargs={\n",
    "        \"temperature\": 0,\n",
    "        \"max_length\": 256,\n",
    "    },  # 모델에 전달할 추가 인자를 설정합니다.\n",
    ")\n",
    "\n",
    "# 프롬프트와 언어 모델을 연결하여 체인을 생성합니다.\n",
    "gpu_chain = prompt | gpu_llm.bind(stop=[\"\\n\\n\"])\n",
    "\n",
    "questions = []\n",
    "for i in range(4):\n",
    "    # 질문 리스트를 생성합니다.\n",
    "    questions.append({\"question\": f\"숫자 {i} 이 한글로 뭐에요?\"})\n",
    "\n",
    "answers = gpu_chain.batch(questions)  # 질문 리스트를 배치 처리하여 답변을 생성합니다.\n",
    "for answer in answers:\n",
    "    print(answer)  # 생성된 답변을 출력합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChatOllama\n",
    "\n",
    "Ollama를 사용하면 Llama 2와 같은 오픈 소스 대규모 언어 모델을 로컬에서 실행할 수 있습니다. Ollama는 모델 가중치, 구성 및 데이터를 Modelfile로 정의된 단일 패키지로 번들링합니다. GPU 사용을 포함하여 설정 및 구성 세부 정보를 최적화합니다. 지원되는 모델 및 모델 변형의 전체 목록은 [Ollama model library](https://ollama.com/library)를 참조하세요.\n",
    "\n",
    "## 설치\n",
    "\n",
    "### 프로그램 설치\n",
    "Ollama를 지원되는 플랫폼(Mac / Linux / Windows)에 다운로드하고 설치하세요.\n",
    "\n",
    "- 설치주소: [https://ollama.com/](https://ollama.com/)\n",
    "\n",
    "### 모델 다운로드\n",
    "\n",
    "#### 허깅페이스\n",
    "\n",
    "허깅페이스(HuggingFace) 에서 오픈모델을 다운로드 받습니다 (.gguf 확장자)\n",
    "\n",
    "- GGUF: https://huggingface.co/teddylee777/EEVE-Korean-Instruct-10.8B-v1.0-gguf\n",
    "\n",
    "#### Ollama 제공하는 모델\n",
    "\n",
    "`ollama pull <name-of-model>` 명령을 사용하여 사용 가능한 LLM 모델을 가져오세요.\n",
    "- 예: `ollama pull gemma:7b`\n",
    "\n",
    "아래의 경로에 모델의 기본 태그 버전이 다운로드됩니다.\n",
    "\n",
    "- Mac: `~/.ollama/models`\n",
    "- Linux/WSL: `/usr/share/ollama/.ollama/models`\n",
    "\n",
    "`ollama list`로 가져온 모든 모델을 확인하세요.\n",
    "\n",
    "`ollama run <name-of-model>`로 명령줄에서 모델과 직접 채팅하세요.\n",
    "\n",
    "### Modelfile 로부터 커스텀 모델 생성하기\n",
    "\n",
    "모델을 임포트하기 위해 ModelFile을 먼저 생성해야 합니다. 자세한 정보는 [ModelFile 관련 공식 문서](https://github.com/ollama/ollama/blob/69f392c9b7ea7c5cc3d46c29774e37fdef51abd8/docs/modelfile.md)에서 확인할 수 있습니다.\n",
    "\n",
    "> 샘플 모델파일 예시\n",
    "\n",
    "```\n",
    "FROM ggml-model-Q5_K_M.gguf\n",
    "\n",
    "TEMPLATE \"\"\"{{- if .System }}\n",
    "<s>{{ .System }}</s>\n",
    "{{- end }}\n",
    "<s>Human:\n",
    "{{ .Prompt }}</s>\n",
    "<s>Assistant:\n",
    "\"\"\"\n",
    "\n",
    "SYSTEM \"\"\"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\"\"\"\n",
    "\n",
    "PARAMETER stop <s>\n",
    "PARAMETER stop </s>\n",
    "```\n",
    "\n",
    "### Chat 모델\n",
    "\n",
    "Llama `chat` 모델(예: `ollama pull llama2:7b-chat`)을 사용하는 경우 `ChatOllama` 인터페이스를 사용할 수 있습니다. 여기에는 시스템 메시지 및 사용자 입력을 위한 special tokens이 포함됩니다.\n",
    "\n",
    "### Ollama 모델 활용\n",
    "\n",
    "- 모든 로컬 모델은 `localhost:11434`에서 제공됩니다.\n",
    "- Command 창에서 직접 상호 작용하려면 `ollama run <name-of-model>`을 실행하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangSmith 추적을 설정합니다. https://smith.langchain.com\n",
    "# !pip install langchain-teddynote\n",
    "from langchain_teddynote import logging\n",
    "\n",
    "# 프로젝트 이름을 입력합니다.\n",
    "logging.langsmith(\"CH04-Models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_teddynote.messages import stream_response\n",
    "\n",
    "# Ollama 모델을 불러옵니다.\n",
    "llm = ChatOllama(model=\"EEVE-Korean-10.8B:latest\")\n",
    "\n",
    "# 프롬프트\n",
    "prompt = ChatPromptTemplate.from_template(\"{topic} 에 대하여 간략히 설명해 줘.\")\n",
    "\n",
    "# 체인 생성\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# 간결성을 위해 응답은 터미널에 출력됩니다.\n",
    "answer = chain.stream({\"topic\": \"deep learning\"})\n",
    "\n",
    "# 스트리밍 출력\n",
    "stream_response(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "비동기 스트리밍(`astream()`) 지원을 위한 예시입니다. 위에서 생성한 단일 chain을 통해 모든 것이 가능합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async for chunks in chain.astream(\n",
    "    {\"topic\": \"구글\"}\n",
    "):  # 비동기적으로 체인을 실행하여 청크 단위로 결과를 반환합니다.\n",
    "    print(chunks, end=\"\", flush=True)  # 각 청크를 출력합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 출력형식: JSON\n",
    "\n",
    "Ollama의 최신 버전을 사용하고 [`format`](https://github.com/jmorganca/ollama/blob/main/docs/api.md#json-mode) 플래그를 제공하세요.\n",
    "\n",
    "`format` 플래그는 모델이 JSON 형식으로 응답을 생성하도록 강제합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=\"gemma:7b\",  # 사용할 언어 모델을 지정합니다.\n",
    "    format=\"json\",  # 입출력 형식을 JSON으로 설정합니다.\n",
    "    temperature=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JSON 형식의 답변을 받기 위해서는 `\"resonse in JSON format.\"` 이 프롬프트에 포함되어야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSON 형식의 답변을 요구하는 프롬프트 작성\n",
    "prompt = \"유럽 여행지 10곳을 알려주세요. key: `places`. resonse in JSON format.\"\n",
    "\n",
    "# 체인 호출\n",
    "response = llm.invoke(prompt)\n",
    "print(response.content)  # 생성된 응답을 출력합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 멀티모달(Multimodal) 지원\n",
    "\n",
    "Ollama는 [bakllava](https://ollama.ai/library/bakllava)와 [llava](https://ollama.ai/library/llava)와 같은 멀티모달 LLM을 지원합니다.\n",
    "\n",
    "`tags`를 사용하여 [Llava](https://ollama.ai/library/llava/tags)와 같은 모델의 전체 버전 세트를 탐색할 수 있습니다.\n",
    "\n",
    "`ollama pull llava:7b` 혹은 `ollama pull bakllava` 명령어를 통해 멀티모달 LLM을 다운로드하세요.\n",
    "\n",
    "**참고**\n",
    "- 멀티모달을 지원하는 최신 버전을 사용하려면 Ollama를 업데이트해야 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PIL 이미지를 Base64 인코딩된 문자열로 변환하고 이를 HTML에 포함하여 이미지를 표시하는 함수를 제공합니다.\n",
    "\n",
    "- `convert_to_base64` 함수:\n",
    "\n",
    "  - PIL 이미지를 입력으로 받습니다.\n",
    "  - 이미지를 JPEG 형식으로 BytesIO 버퍼에 저장합니다.\n",
    "  - 버퍼의 값을 Base64로 인코딩하고 문자열로 반환합니다.\n",
    "\n",
    "- `plt_img_base64` 함수:\n",
    "\n",
    "  - Base64 인코딩된 문자열을 입력으로 받습니다.\n",
    "  - Base64 문자열을 소스로 사용하는 HTML `<img>` 태그를 생성합니다.\n",
    "  - HTML을 렌더링하여 이미지를 표시합니다.\n",
    "\n",
    "- 사용 예시:\n",
    "  - 지정된 파일 경로에서 PIL 이미지를 열어 `pil_image`에 저장합니다.\n",
    "  - `convert_to_base64` 함수를 사용하여 `pil_image`를 Base64 인코딩된 문자열로 변환합니다.\n",
    "  - `plt_img_base64` 함수를 사용하여 Base64 인코딩된 문자열을 이미지로 표시합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "from io import BytesIO\n",
    "\n",
    "from IPython.display import HTML, display\n",
    "from PIL import Image\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "\n",
    "def convert_to_base64(pil_image):\n",
    "    \"\"\"\n",
    "    PIL 이미지를 Base64로 인코딩된 문자열로 변환합니다.\n",
    "\n",
    "    :param pil_image: PIL 이미지\n",
    "    :return: 크기 조정된 Base64 문자열\n",
    "    \"\"\"\n",
    "\n",
    "    buffered = BytesIO()\n",
    "    pil_image.save(buffered, format=\"JPEG\")  # 필요한 경우 형식을 변경할 수 있습니다.\n",
    "    img_str = base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
    "    return img_str\n",
    "\n",
    "\n",
    "def plt_img_base64(img_base64):\n",
    "    \"\"\"\n",
    "    Base64로 인코딩된 문자열을 이미지로 표시합니다.\n",
    "\n",
    "    :param img_base64:  Base64 문자열\n",
    "    \"\"\"\n",
    "    # Base64 문자열을 소스로 사용하여 HTML img 태그 생성\n",
    "    image_html = f'<img src=\"data:image/jpeg;base64,{img_base64}\" />'\n",
    "    # HTML을 렌더링하여 이미지 표시\n",
    "    display(HTML(image_html))\n",
    "\n",
    "\n",
    "def prompt_func(data):  # 프롬프트 함수를 정의합니다.\n",
    "    text = data[\"text\"]  # 데이터에서 텍스트를 가져옵니다.\n",
    "    image = data[\"image\"]  # 데이터에서 이미지를 가져옵니다.\n",
    "\n",
    "    image_part = {  # 이미지 부분을 정의합니다.\n",
    "        \"type\": \"image_url\",  # 이미지 URL 타입을 지정합니다.\n",
    "        \"image_url\": f\"data:image/jpeg;base64,{image}\",  # 이미지 URL을 생성합니다.\n",
    "    }\n",
    "\n",
    "    content_parts = []  # 콘텐츠 부분을 저장할 리스트를 초기화합니다.\n",
    "\n",
    "    text_part = {\"type\": \"text\", \"text\": text}  # 텍스트 부분을 정의합니다.\n",
    "\n",
    "    content_parts.append(image_part)  # 이미지 부분을 콘텐츠 부분에 추가합니다.\n",
    "    content_parts.append(text_part)  # 텍스트 부분을 콘텐츠 부분에 추가합니다.\n",
    "\n",
    "    return [HumanMessage(content=content_parts)]  # HumanMessage 객체를 반환합니다.\n",
    "\n",
    "\n",
    "file_path = \"./images/jeju-beach.jpg\"\n",
    "pil_image = Image.open(file_path)\n",
    "\n",
    "image_b64 = convert_to_base64(pil_image)\n",
    "\n",
    "plt_img_base64(image_b64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `ChatOllama` 언어 모델을 사용하여 이미지와 텍스트 기반 질의에 대한 답변을 생성하는 체인을 구현합니다.\n",
    "- `prompt_func` 함수는 이미지와 텍스트 데이터를 입력으로 받아 `HumanMessage` 형식으로 변환합니다.\n",
    "  - 이미지 데이터는 Base64 인코딩된 JPEG 형식으로 전달됩니다.\n",
    "  - 텍스트 데이터는 일반 텍스트로 전달됩니다.\n",
    "- `StrOutputParser`를 사용하여 언어 모델의 출력을 문자열로 파싱합니다.\n",
    "- `prompt_func`, `llm`, `StrOutputParser`를 파이프라인으로 연결하여 `chain`을 생성합니다.\n",
    "- `chain.invoke` 메서드를 호출하여 이미지와 텍스트 질의를 전달하고 답변을 생성합니다.\n",
    "- 생성된 답변을 출력합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# ChatOllama 멀티모달 언어 모델을 불러옵니다.\n",
    "llm = ChatOllama(model=\"llava:7b\", temperature=0)\n",
    "\n",
    "# 프롬프트 함수, 언어 모델, 출력 파서를 연결하여 체인을 생성합니다.\n",
    "chain = prompt_func | llm | StrOutputParser()\n",
    "\n",
    "query_chain = chain.invoke(  # 체인을 호출하여 쿼리를 실행합니다.\n",
    "    # 텍스트와 이미지를 전달합니다.\n",
    "    {\"text\": \"Describe a picture in bullet points\", \"image\": image_b64}\n",
    ")\n",
    "\n",
    "print(query_chain)  # 쿼리 결과를 출력합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT4All\n",
    "\n",
    "![](./images/gpt4all.png)\n",
    "\n",
    "[GitHub:nomic-ai/gpt4all](https://github.com/nomic-ai/gpt4all) 은 코드, 채팅 형식의 대화를 포함한 방대한 양의 데이터로 학습된 오픈 소스 챗봇 생태계입니다.\n",
    "\n",
    "이 예제에서는 LangChain을 사용하여 `GPT4All` 모델과 상호 작용하는 방법에 대해 설명합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 설치방법\n",
    "\n",
    "1. 먼저, 공식 홈페이지에 접속하여 설치파일을 다운로드 받아 설치합니다\n",
    "2. [공식 홈페이지](https://gpt4all.io/index.html) 바로가기\n",
    "3. 파이썬 패키지를 설치합니다.\n",
    "4. [pip 를 활용한 설치 방법](https://github.com/nomic-ai/gpt4all/blob/main/gpt4all-bindings/python/README.md)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -qU gpt4all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 다운로드\n",
    "\n",
    "![](./images/gpt4all_models.png)\n",
    "\n",
    "[gpt4all 페이지](https://gpt4all.io/index.html)에는 `Model Explorer` 섹션이 있습니다.\n",
    "(더 많은 정보를 원하시면 https://github.com/nomic-ai/gpt4all 을 방문하세요.)\n",
    "\n",
    "1. [공식 홈페이지](https://gpt4all.io/index.html) 에서 다운로드 가능한 모델을 다운로드 받습니다. 본인의 PC 사양에서 구동가능한 모델을 선택하는 것이 좋습니다.\n",
    "2. 본 튜토리얼에서는 `EEVE-Korean-Instruct-10.8B-v1.0-Q8_0.gguf`(10.69GB) 모델을 다운로드 받아 진행하겠습니다.\n",
    "3. 다운로드 받은 모델은 `models` 폴더 생성 후 해당 폴더에 다운로드 받습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `local_path` 변수에 로컬 파일 경로(`\"./models/EEVE-Korean-Instruct-10.8B-v1.0-Q8_0.gguf\"`)를 할당합니다.\n",
    "- 이 경로는 사용자가 원하는 로컬 파일 경로로 대체할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_path = \"./models/EEVE-Korean-Instruct-10.8B-v1.0-Q8_0.gguf\"  # 원하는 로컬 파일 경로로 대체하세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 정보 설정\n",
    "\n",
    "로컬에서 실행하려면 호환되는 ggml 형식의 모델을 다운로드하세요.\n",
    "\n",
    "- 관심 있는 모델을 선택하세요.\n",
    "- UI를 사용하여 다운로드하고 `.bin` 파일을 `local_path`(아래 참고)로 이동시키세요.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT4ALL 모델 활용\n",
    "\n",
    "`GPT4All`은 GPT-3와 유사한 대규모 언어 모델로, 다양한 자연어 처리 작업에 활용될 수 있습니다.\n",
    "\n",
    "이 모듈을 사용하면 GPT4All 모델을 간편하게 로드하고 추론에 활용할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_community.llms import GPT4All\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.callbacks import StreamingStdOutCallbackHandler\n",
    "\n",
    "# 프롬프트\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"<s>A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.</s>\n",
    "<s>Human: {question}</s>\n",
    "<s>Assistant:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# GPT4All 언어 모델 초기화\n",
    "# model는 GPT4All 모델 파일의 경로를 지정\n",
    "llm = GPT4All(\n",
    "    model=local_path,\n",
    "    backend=\"gpu\",  # GPU 설정\n",
    "    streaming=True,  # 스트리밍 설정\n",
    "    callbacks=[StreamingStdOutCallbackHandler()],  # 콜백 설정\n",
    ")\n",
    "\n",
    "# 체인 생성\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# 질의 실행\n",
    "response = chain.invoke({\"question\": \"대한민국의 수도는 어디인가요?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 비디오(Video) 질의 응답 LLM (Gemini)\n",
    "\n",
    "**주요 흐름**\n",
    "\n",
    "1. `File API`를 사용하여 비디오 파일을 업로드합니다.\n",
    "2. `GenerateContent` 요청을 통해 비디오에 대한 질문을 요청합니다.\n",
    "3. 생성된 응답을 확인합니다.\n",
    "4. 업로드한 Video 파일을 삭제합니다.\n",
    "\n",
    "\n",
    "**중요:** \n",
    "\n",
    "- 본 튜토리얼의 `File API` 는 인증 및 접근을 위해 `API keys`를 사용합니다. \n",
    "\n",
    "- 업로드된 파일은 `API key`의 클라우드 프로젝트와 연결됩니다. \n",
    "\n",
    "다른 `Gemini API` 와 달리, `API key`는 `File API`에 업로드한 데이터에 대한 접근 권한도 부여하므로 `API key`를 안전하게 보관하는 데 특별히 주의해야 합니다.\n",
    "\n",
    "**Reference**\n",
    "\n",
    "- [Gemini API(Cookbook) - Video](https://ai.google.dev/api/rest/v1/models/generateContent#media)\n",
    "\n",
    "**API KEY 발급**\n",
    "\n",
    "- [링크](https://makersuite.google.com/app/apikey?hl=ko) 에서 API KEY를 발급받아주세요.\n",
    "- 사용자의 Google API 키를 환경 변수 `GOOGLE_API_KEY`로 설정합니다.\n",
    "\n",
    "`.env` 파일에 아래와 같이 입력합니다.\n",
    "\n",
    "```\n",
    "GOOGLE_API_KEY=<사용자의 API KEY>\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith 추적을 시작합니다.\n",
      "[프로젝트명]\n",
      "CH04-Models\n"
     ]
    }
   ],
   "source": [
    "# LangSmith 추적 설정\n",
    "# !pip install langchain-teddynote\n",
    "from langchain_teddynote import logging\n",
    "\n",
    "# 프로젝트 이름 입력\n",
    "logging.langsmith(\"CH04-Models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith 추적을 시작합니다.\n",
      "[프로젝트명]\n",
      "CH04-Gemini-Video\n"
     ]
    }
   ],
   "source": [
    "# LangSmith 추적 설정\n",
    "# !pip install langchain-teddynote\n",
    "from langchain_teddynote import logging\n",
    "\n",
    "# 프로젝트 이름 입력\n",
    "logging.langsmith(\"CH04-Gemini-Video\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 비디오 업로드\n",
    "\n",
    "`Gemini API` 는 비디오 파일 형식을 직접 수용합니다. \n",
    "\n",
    "**제한사항**\n",
    "\n",
    "- `File API`는 2GB 이하의 파일을 수용하며 프로젝트당 최대 20GB의 파일을 저장할 수 있습니다. \n",
    "- 파일은 2일 동안 유지되며 API에서 **다운로드할 수 없습니다**. \n",
    "\n",
    "본 예제는 테디노트 YouTube 채널에 게시된 비디오를 사용합니다. (다른 비디오로 교체하여 진행해도 좋습니다)\n",
    "\n",
    "- [🧑‍💻 #PDF 전처리 할 수 있는 건 다 해봤음.](https://youtu.be/O3qFWRObAXw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일 다운로드 후 teddynote-sample-video.mp4 파일로 저장\n",
    "!wget \"https://www.dropbox.com/scl/fi/ugue14fyo010jgc7wuh4g/teddynote-sample-video.mp4?rlkey=wcsiktklt7jgoibsluft3m6z9&st=prv4p2uu&dl=1\" -qO teddynote-sample-video.mp4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 비디오 파일의 경로를 입력합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 비디오 파일 이름 지정\n",
    "video_file_name = \"teddynote-sample-video.mp4\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음으로는 `File API`를 사용하여 비디오 파일을 업로드합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "파일을 업로드 중입니다...\n",
      "업로드 완료: https://generativelanguage.googleapis.com/v1beta/files/b1d0iktswld\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "# 파일 업로드 진행 메시지 출력\n",
    "print(f\"파일을 업로드 중입니다...\")\n",
    "\n",
    "# 파일 업로드 및 파일 객체 반환\n",
    "video_file = genai.upload_file(path=video_file_name)\n",
    "\n",
    "# 업로드 완료 메시지 및 파일 URI 출력\n",
    "print(f\"업로드 완료: {video_file.uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "파일을 업로드한 후, `files.get` 을 호출하여 API가 파일을 성공적으로 완료되었는지 확인할 수 있습니다. \n",
    "\n",
    "`files.get`은  API 키가 속한 클라우드 프로젝트와 연관된 파일 API에 업로드된 파일을 확인할 수 있게 해줍니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "비디오 업로드 및 전처리가 완료될 때까지 잠시만 기다려주세요...\n",
      "비디오 업로드 및 전처리가 완료될 때까지 잠시만 기다려주세요...\n",
      "비디오 업로드 및 전처리가 완료될 때까지 잠시만 기다려주세요...\n",
      "\n",
      "비디오 처리가 완료되었습니다!\n",
      "이제 대화를 시작할 수 있어요: https://generativelanguage.googleapis.com/v1beta/files/b1d0iktswld\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# 비디오 파일 처리 상태 확인\n",
    "while video_file.state.name == \"PROCESSING\":\n",
    "    # 처리 완료 대기 메시지 출력\n",
    "    print(\"비디오 업로드 및 전처리가 완료될 때까지 잠시만 기다려주세요...\")\n",
    "    # 10초 대기\n",
    "    time.sleep(10)\n",
    "    # 비디오 파일 상태 갱신\n",
    "    video_file = genai.get_file(video_file.name)\n",
    "\n",
    "# 처리 실패 시 예외 발생\n",
    "if video_file.state.name == \"FAILED\":\n",
    "    raise ValueError(video_file.state.name)\n",
    "\n",
    "# 처리 완료 메시지 출력\n",
    "print(\n",
    "    f\"\\n비디오 처리가 완료되었습니다!\\n이제 대화를 시작할 수 있어요: \" + video_file.uri\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "비디오가 업로드된 후, `generate_content` 함수를 사용하여 Video 에 대한 질문을 요청할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이 영상은 PDF 파일을 파싱하는 방법에 대한 튜토리얼입니다. 랭체인에서 PDF 파일을 파싱할 때 레이아웃 분석을 사용하는 방법과 업스테이지 레이아웃 분석 알고리즘을 사용하는 방법을 설명합니다. 또한 라마 파서를 사용하여 PDF 파일을 파싱하고 이미지, 테이블, 텍스트 등을 추출하는 방법을 보여줍니다. \n",
      " 마지막으로, 랭체인에서 여러 PDF 페이지를 하나의 마크다운 파일로 합쳐서 처리하는 방법을 보여줍니다. \n",
      " \n",
      " 튜토리얼을 통해 PDF 파일을 파싱하는 방법을 배우고 랭체인을 더 효과적으로 사용하는 방법을 익힐 수 있습니다. \n"
     ]
    }
   ],
   "source": [
    "# 프롬프트\n",
    "prompt = \"이 영상에 대해서 짧게 요약해 줄 수 있나요?\"\n",
    "\n",
    "# 모델을 Gemini 1.5 Flash로 설정\n",
    "model = genai.GenerativeModel(model_name=\"models/gemini-1.5-flash\")\n",
    "\n",
    "# LLM 답변 요청\n",
    "response = model.generate_content(\n",
    "    [prompt, video_file], request_options={\"timeout\": 600}\n",
    ")\n",
    "# 결과 출력\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래는 스트림 출력 예제입니다. (`stream=True` 옵션 추가)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gencon에 대한 언급은 0:27부터 나옵니다. \n",
      "\n",
      "\"오늘 또 이벤트가 있습니다. 젠콘, 젠콘 이벤트 무료로 세 분께 드리는 이벤트 하거든요. 중간에 퀴즈를 낼 건데. 중간에 아무를 제가 말씀드릴 거예요. 아, 라이브에 들으시는 분들은 아무를 쉽게 가실 수 있겠는데 설문에다 작성해주시면 됩니다. 그래서 9월 20일 날 혹시 시간 되시는 분들은 여기 오셔 가지고 들으시면은 또 도움이 될 것 같아요.\" \n"
     ]
    }
   ],
   "source": [
    "# 프롬프트 생성\n",
    "prompt = \"이 영상에서 Gencon 관련 언급한 부분의 시간을 알려주고, 어떤 내용을 말했는지 알려주세요.\"\n",
    "\n",
    "# 모델을 Gemini 1.5 Flash로 설정\n",
    "model = genai.GenerativeModel(model_name=\"models/gemini-1.5-flash\")\n",
    "\n",
    "# LLM 스트림 답변 요청\n",
    "response = model.generate_content(\n",
    "    [prompt, video_file], request_options={\"timeout\": 600}, stream=True\n",
    ")\n",
    "\n",
    "# 생성된 콘텐츠 출력\n",
    "for chunk in response:\n",
    "    print(chunk.text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 파일 삭제\n",
    "\n",
    "파일은 2일 후 자동으로 삭제되거나 `files.delete()`를 사용하여 수동으로 삭제할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영상을 삭제했습니다: https://generativelanguage.googleapis.com/v1beta/files/b1d0iktswld\n"
     ]
    }
   ],
   "source": [
    "# 파일 삭제\n",
    "genai.delete_file(video_file.name)\n",
    "\n",
    "# 삭제 완료 메시지 출력\n",
    "print(f\"영상을 삭제했습니다: {video_file.uri}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
